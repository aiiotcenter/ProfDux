[{"id":"m2vvuog8","question":{"english":"Attention Mechanisms are a key component in the Transformers model.","turkish":"Dikkat Mekanizmalar\u0131, Transformers modelinde temel bir bile\u015fendir."},"answerOptions":{"english":["True","False"],"turkish":["Do\u011fru","Yanl\u0131\u015f"]},"answer":{"english":"True","turkish":"Do\u011fru"},"type":"TrueAndFalseQuestion","hardness":"medium","marksWorth":1},{"id":"m2vvubsd","question":{"english":"What is the term used to describe the scaling technique used in Transformers to handle long sequences efficiently?","turkish":"Transformer' \u0131n uzun dizileri etkili bir \u015fekilde ele almak i\u00e7in kulland\u0131\u011f\u0131 \u00f6l\u00e7ekleme tekni\u011fi i\u00e7in kullan\u0131lan terim nedir?"},"answerOptions":{"english":["Query Expansion","Feed-Forward Networks","Self-Attention","Embedding Layers"],"turkish":["Sorgu Geni\u015fletme","\u0130leri Besleme A\u011flar\u0131","\u00d6z-Dikkat","G\u00f6mme Katmanlar\u0131"]},"answer":{"english":"Self-Attention","turkish":"\u00d6z-Dikkat"},"type":"MultipleChoiceQuestion","hardness":"Extreme","marksWorth":1},{"id":"m2vvueu3","question":{"english":"FillInTheBlankQuestion","turkish":"FillInTheBlankQuestion"},"answer":{"english":"answer for question 2","turkish":"Turkish answer for question 2"},"type":"FillInTheBlankQuestion","hardness":"medium","marksWorth":1},{"id":"m2vvuex4","question":{"english":"Which component in a Transformer model enables capturing dependencies regardless of their distance in the input sequence?","turkish":"Bir Transformer modelinde hangi bile\u015fen, giri\u015f dizisindeki uzakl\u0131klar\u0131ndan ba\u011f\u0131ms\u0131z olarak ba\u011f\u0131ml\u0131l\u0131klar\u0131 yakalamay\u0131 sa\u011flar?"},"answerOptions":{"english":["Feed-Forward Sublayer","Positional Encoding","Layer Normalization","Multi-Head Self-Attention"],"turkish":["\u0130leri Besleme Alt Katman","Pozisyon Kodlama","Katman Normalizasyonu","\u00c7ok Ba\u015fl\u0131 \u00d6z-Dikkat"]},"answer":{"english":"Multi-Head Self-Attention","turkish":"\u00c7ok Ba\u015fl\u0131 \u00d6z-Dikkat"},"type":"MultipleChoiceQuestion","hardness":"Extreme","marksWorth":1},{"id":"m2vvukwt","question":{"english":"Transformers have eliminated the need for sequential processing in NLP tasks.","turkish":"Transformers, NLP g\u00f6revlerinde s\u0131ral\u0131 i\u015flemeye gerek olmad\u0131\u011f\u0131n\u0131 ortadan kald\u0131rm\u0131\u015ft\u0131r."},"answerOptions":{"english":["True","False"],"turkish":["Do\u011fru","Yanl\u0131\u015f"]},"answer":{"english":"False","turkish":"Yanl\u0131\u015f"},"type":"TrueAndFalseQuestion","hardness":"medium","marksWorth":1},{"id":"m2vvuktd","question":{"english":"The Transformers architecture is commonly used in NLP tasks.","turkish":"Transformers mimarisi genellikle NLP g\u00f6revlerinde kullan\u0131l\u0131r."},"answerOptions":{"english":["True","False"],"turkish":["Do\u011fru","Yanl\u0131\u015f"]},"answer":{"english":"True","turkish":"Do\u011fru"},"type":"TrueAndFalseQuestion","hardness":"medium","marksWorth":1},{"id":"m2vvuldc","question":{"english":"FillInTheBlankQuestion","turkish":"FillInTheBlankQuestion"},"answer":{"english":"answer for question 3","turkish":"Turkish answer for question 3"},"type":"FillInTheBlankQuestion","hardness":"medium","marksWorth":1},{"id":"m2vvufsp","question":{"english":"FillInTheBlankQuestion","turkish":"FillInTheBlankQuestion"},"answer":{"english":"answer for question 4","turkish":"Turkish answer for question 4"},"type":"FillInTheBlankQuestion","hardness":"medium","marksWorth":1},{"id":"m2vvuerg","question":{"english":"The concept of Attention Mechanisms is crucial in _____________.","turkish":"Dikkat Mekanizmalar\u0131 kavram\u0131 _____________'da hayati bir \u00f6neme sahiptir."},"answer":{"english":"Transformers","turkish":"D\u00f6n\u00fc\u015ft\u00fcr\u00fcc\u00fcler"},"type":"FillInTheBlankQuestion","hardness":"medium","marksWorth":1},{"id":"m2vvunvh","question":{"english":"Attention Mechanisms allow models to focus on relevant parts of input sequences.","turkish":"Dikkat mekanizmalar\u0131, modellerin giri\u015f dizilerinin ilgili k\u0131s\u0131mlar\u0131na odaklanmas\u0131na izin verir."},"answerOptions":{"english":["True","False"],"turkish":["Do\u011fru","Yanl\u0131\u015f"]},"answer":{"english":"True","turkish":"Do\u011fru"},"type":"TrueAndFalseQuestion","hardness":"medium","marksWorth":1},{"id":"m2vvucgb","question":{"english":"In Attention Mechanisms, what is the purpose of the query, key, and value vectors?","turkish":"Dikkat Mekanizmalar\u0131nda, sorgu, anahtar ve de\u011fer vekt\u00f6rlerinin amac\u0131 nedir?"},"answerOptions":{"english":["Computing the Similarity","Adjusting Weights","Memory Retrieval","Output Calculation"],"turkish":["Benzerli\u011fin Hesaplanmas\u0131","A\u011f\u0131rl\u0131klar\u0131n Ayarlanmas\u0131","Bellekten Geri Al\u0131m","\u00c7\u0131k\u0131\u015f\u0131n Hesaplanmas\u0131"]},"answer":{"english":"Computing the Similarity","turkish":"Benzerli\u011fin Hesaplanmas\u0131"},"type":"MultipleChoiceQuestion","hardness":"Extreme","marksWorth":1},{"id":"m2vvuehg","question":{"english":"What is the primary motivation behind the development of Transformers to replace recurrent neural networks for NLP tasks?","turkish":"Rek\u00fcrrensin sinir a\u011flar\u0131n\u0131 NLP g\u00f6revleri i\u00e7in de\u011fi\u015ftirmek amac\u0131yla Transformer'\u0131n geli\u015ftirilmesindeki temel motivasyon nedir?"},"answerOptions":{"english":["Efficiency in Parallelization","Sequential Computation","Temporal Dependency Modeling","Gradient Vanishing Prevention"],"turkish":["Paralelle\u015ftirme Verimi","S\u0131ral\u0131 Hesaplama","Zamansal Ba\u011f\u0131ml\u0131l\u0131k Modelleme","Gradyan Kaybolma \u00d6nleme"]},"answer":{"english":"Efficiency in Parallelization","turkish":"Paralelle\u015ftirme Verimi"},"type":"MultipleChoiceQuestion","hardness":"Extreme","marksWorth":1}]